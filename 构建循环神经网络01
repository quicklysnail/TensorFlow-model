# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import matplotlib.pyplot as plt

mnist = input_data.read_data_sets('./mnist', one_hot=True)      # they has been normalized to range (0,1)
mnist.test.images
print(mnist.train.images.shape)
print(mnist.train.labels.shape)

test_x = mnist.test.images[:2000]
test_y = mnist.test.labels[:2000]             

plt.imshow(mnist.train.images[0].reshape((28, 28)), cmap='gray')
plt.title('%i' % np.argmax(mnist.train.labels[0]))
plt.show()


tf.set_random_seed(1)
np.random.seed(1)

lr =0.001
training_iters = 100000
batch_size = 128

n_inputs = 28
n_steps =28
n_hidden_unis =128
n_classes = 10


x = tf.placeholder(tf.float32 , [None , n_steps * n_inputs])     # shape(batch, 784)
image = tf.reshape(x, [-1, n_steps , n_inputs])              
y = tf.placeholder(tf.int32 , [None , n_classes])            # input y


Weights = {'in':tf.Variable(tf.random_normal([n_inputs ,n_hidden_unis])),'out':tf.Variable(tf.random_normal([n_hidden_unis , n_classes]))}
biases = {'in':tf.Variable(tf.constant(0.1 , shape = [n_hidden_unis,])) , 'out':tf.Variable(tf.constant(0.1 , shape=[n_classes,]))}


def RNN(image , Weights ,biases):
    global outputs
    global h_c
    global h_n
    #X = tf.reshape(X , [-1 , n_inputs  ])
    #X_in = tf.matmul(X , Weights['in']) + biases['in']
    #X_in = tf.reshape(X_in , [-1 , n_steps , n_hidden_unis])
    
    
    ###  传入cell
    rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden_unis)
    ### 不用设定初始值为零initial_state
    outputs, (h_c, h_n) = tf.nn.dynamic_rnn(
    rnn_cell,                   # cell you have chosen
    image,                      # input
    initial_state=None,         # the initial hidden state
    dtype=tf.float32,           # must given if set initial_state = None
    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)
    )   ##### 这个RNN的结构弄清楚内部到底有没有（WX）

    results = tf.matmul(h_n , Weights['out']) + biases['out']   ### 这一步没有softmax激活
    return results

pred = RNN(image,Weights , biases)
loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=pred)           # compute cost
train_step = tf.train.AdamOptimizer(lr).minimize(loss)

correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred , tf.float32))

init =tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    step = 0
    while step*batch_size < training_iters:
        batch_xs , batch_ys =mnist.train.next_batch(batch_size)
        #batch_xs = batch_xs.reshape([batch_size , n_steps ,n_inputs])
        sess.run(train_step ,feed_dict={x:batch_xs , y:batch_ys})
        if step%20 ==0:
            
            print('train loss:',sess.run(loss , {x:batch_xs , y:batch_ys}))
            print('test loss:',sess.run(loss , {x:test_x , y:test_y}))
            #print(tf.shape(sess.run(outputs , {x:batch_xs , y:batch_ys})))
            #print(tf.shape(sess.run(h_c , {x:batch_xs , y:batch_ys})))
            #print(tf.shape(sess.run(h_n , {x:batch_xs , y:batch_ys})))
            
            print('test accuracy:',sess.run(accuracy , {x:test_x , y:test_y}))
            
        step +=1


